{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225af349",
   "metadata": {},
   "source": [
    "# Usable UMIs Amount Estimation\n",
    "To avoid sequencing and PCR errors, we use only UMIs with multiple duplicate reads. To determine a required sequencing depth (number of reads per cell), we would like to model the relationship between sequencing depth and number of UMIs present in given number of duplicates.\n",
    "\n",
    "We use dataset obtained from mouse kidneys, sequenced by 10x sequencing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d3e8987",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'support_estimation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses_json\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataClassJsonMixin\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msupport_estimation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Support\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'support_estimation'"
     ]
    }
   ],
   "source": [
    "from collections import Counter, OrderedDict, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from dataclasses_json import DataClassJsonMixin\n",
    "from support_estimation.support import Support\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e451318",
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_files_data_folder = Path('/cellfile/datapublic/jkoubele/DR_dataset/AL4_old', desc='Processing .bam files')\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CellStatistics(DataClassJsonMixin):\n",
    "    barcode: str\n",
    "    num_reads: int\n",
    "    duplicate_counter: dict[int, int]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Dataset(DataClassJsonMixin):\n",
    "    cells: list[CellStatistics]\n",
    "\n",
    "\n",
    "run_processing_of_bam_files = False  # change to True to re-run the .bam files processing\n",
    "if run_processing_of_bam_files:\n",
    "    cells: list[CellStatistics] = []\n",
    "    for file_path in tqdm(bam_files_data_folder.iterdir()):\n",
    "        if file_path.suffix != '.bam':\n",
    "            continue\n",
    "        samfile = pysam.AlignmentFile(file_path, \"rb\")\n",
    "        reads = [read for read in samfile if read.mapq >= 255 and\n",
    "                 read.has_tag('UB') and\n",
    "                 read.get_tag('UB') != '-' and\n",
    "                 read.reference_name != 'chrM']\n",
    "        umi_counter = Counter([read.get_tag('UB') for read in reads])\n",
    "        cells.append(CellStatistics(barcode=file_path.stem,\n",
    "                                    num_reads=len(reads),\n",
    "                                    duplicate_counter=dict(Counter(umi_counter.values()))\n",
    "                                    ))\n",
    "\n",
    "    with open(f'./duplicate_counters_{bam_files_data_folder.stem}.json', 'w') as file_out:\n",
    "        file_out.write(Dataset(cells=cells).to_json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87ebce",
   "metadata": {},
   "source": [
    "The median sequencing depth was 51k reads, and median number of UMIs was 22k.\n",
    "\n",
    "We plot the histograms of reads per cell and also UMIs per cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc430464",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./duplicate_counters_AL4_old.json') as input_file:\n",
    "    dataset = Dataset.from_json(input_file.read())\n",
    "    cells = dataset.cells\n",
    "\n",
    "num_reads = [cell.num_reads for cell in cells]\n",
    "\n",
    "plt.hist(num_reads, bins=50)\n",
    "plt.xlabel('Num. reads in cell')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of number of reads in cells')\n",
    "plt.savefig('histogram_reads.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Descriptive statistics:\\n\", pd.Series(num_reads).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3337920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_umis = [sum(cell.duplicate_counter.values()) for cell in cells]\n",
    "\n",
    "plt.hist(num_umis, bins=50)\n",
    "plt.xlabel('Num. UMIs in cell')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of number of UMIs in cells')\n",
    "plt.savefig('histogram_umis.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Descriptive statistics:\\n\", pd.Series(num_umis).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076554e",
   "metadata": {},
   "source": [
    "Although cells somehow vary in the number of reads, the fraction of UMIs with a given number of duplicates is quite stable across the cells. E.g, most cells have about 40% of UMIs that have only 1 read, 25% of UMIs with 2 reads, etc.\n",
    "\n",
    "This suggests that the differences in number of reads is driven primarily by cell size / mRNA content, and not by different capturing efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6119f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))\n",
    "plt.tight_layout(pad=2)\n",
    "for duplicate_number in range(1, 10):\n",
    "    plt.subplot(3, 3, duplicate_number)\n",
    "    plt.hist([(cell.duplicate_counter[duplicate_number] if duplicate_number in cell.duplicate_counter else 0)\n",
    "              / sum(cell.duplicate_counter.values())\n",
    "              for cell in cells], bins=20)\n",
    "    plt.title(f'Fraction of UMIs with {duplicate_number} duplicates')\n",
    "    plt.ylabel('Cell count')\n",
    "\n",
    "plt.savefig('umi_fractions_per_duplicate.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c93901",
   "metadata": {},
   "source": [
    "We may therefore aggregate the reads from all cells together, to get a distribution of UMI duplicates in an average or median cell. \n",
    "\n",
    "As cells contain only negligible amount of UMIs that have more than 15 duplicates, we will omit them to simplify the analysis (and make plotting easier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_numbers = set(chain.from_iterable([cell.duplicate_counter.keys() for cell in cells]))\n",
    "average_cell_duplicate_counter = {duplicate_number:\n",
    "                                      sum([cell.duplicate_counter[\n",
    "                                               duplicate_number] if duplicate_number in cell.duplicate_counter else 0\n",
    "                                           for cell in cells]) / len(cells)\n",
    "                                  for duplicate_number in duplicate_numbers}\n",
    "print(\n",
    "    f\"Average number of UMIs with more than 15 duplicates: {sum([value for key, value in average_cell_duplicate_counter.items() if key > 15])}\")\n",
    "average_cell_duplicate_counter = {key: value for key, value in average_cell_duplicate_counter.items() if key <= 15}\n",
    "average_cell_duplicate_counter = OrderedDict(\n",
    "    {key: average_cell_duplicate_counter[key] for key in sorted(average_cell_duplicate_counter.keys())})\n",
    "print(f\"Number of reads in average cell: {sum([key * value for key, value in average_cell_duplicate_counter.items()])}\")\n",
    "print(f\"Number of UMIs in average cell: {sum(average_cell_duplicate_counter.values())}\")\n",
    "\n",
    "median_cell_duplicate_counter = OrderedDict({duplicate_number:\n",
    "                                                 np.median([cell.duplicate_counter[\n",
    "                                                                duplicate_number] if duplicate_number in cell.duplicate_counter else 0\n",
    "                                                            for cell in cells])\n",
    "                                             for duplicate_number in range(1, 16)})\n",
    "\n",
    "median_cell_num_reads = sum([key * value for key, value in median_cell_duplicate_counter.items()])\n",
    "median_cell_num_umis = sum(median_cell_duplicate_counter.values())\n",
    "print(f\"Number of reads in median cell: {median_cell_num_reads}\")\n",
    "print(f\"Number of UMIs in median cell: {median_cell_num_umis}\")\n",
    "\n",
    "plt.bar(median_cell_duplicate_counter.keys(), height=median_cell_duplicate_counter.values())\n",
    "plt.title('UMIs duplicates in median cell')\n",
    "plt.ylabel('UMIs')\n",
    "plt.xlabel('Num. duplicates')\n",
    "plt.savefig('umi_duplicate_median_cell_histogram.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39342bd7",
   "metadata": {},
   "source": [
    "# Estimating the Number of Captured UMIs\n",
    "We will use the method from paper [Chebyshev polynomials, moment matching, and optimal estimation of the unseen](https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Chebyshev-polynomials-moment-matching-and-optimal-estimation-of-the-unseen/10.1214/17-AOS1665.full) to estimate the number of UMIs that were captured, but never sampled.\n",
    "\n",
    "The method contain one parameter - $k$, the maximum plausible size of UMI pool. The method is assuming that the mimimum probability for UMI being sampled (if it's captured in the first place) is at least $\\frac{1}{k}$. \n",
    "\n",
    "Fortunatelly, the estimated number of UMI pool size is relatively insensitive to the parameter $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a884d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint = [[key, value] for key, value in median_cell_duplicate_counter.items() if value > 0 and key > 0]\n",
    "\n",
    "k_values = np.arange(20_000, 1000_000, 1000)\n",
    "estimated_support_sizes = [Support(pmin=1 / k).estimate(fingerprint) for k in k_values]\n",
    "plt.plot(k_values, estimated_support_sizes)\n",
    "plt.xlabel('Max plausible UMIs')\n",
    "plt.ylabel('Estimated support size')\n",
    "plt.savefig('estimated_support_size_vs_max_plausible_size.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Support(pmin=1 / 90_000).estimate(fingerprint) - median_cell_num_umis)\n",
    "print(Support(pmin=1 / 360_000).estimate(fingerprint) - median_cell_num_umis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4715b1",
   "metadata": {},
   "source": [
    "We will set $k=2\\times10^5$ as a maximum plausible number of captured UMIs. This leads to an estimate of support size equal to 33559, indicating that there is 11457 UMIs that were captured but never sampled during the sequencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df7f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_support_size = int(Support(pmin=1 / 200_000).estimate(fingerprint))\n",
    "print(f\"{estimated_support_size=}\")\n",
    "median_cell_duplicate_counter[0] = int(estimated_support_size - median_cell_num_umis)\n",
    "sum(median_cell_duplicate_counter.values())\n",
    "median_cell_duplicate_counter\n",
    "\n",
    "plt.bar(median_cell_duplicate_counter.keys(), height=median_cell_duplicate_counter.values(), label='observed')\n",
    "plt.bar(0, height=median_cell_duplicate_counter[0], color='lightblue', label='unobserved (estimate)')\n",
    "plt.title('UMIs duplicates in median cell + estimate of unseen UMIs')\n",
    "plt.ylabel('UMIs')\n",
    "plt.xlabel('Num. duplicates')\n",
    "plt.legend()\n",
    "plt.savefig('umi_duplicate_with_unseen_estimate.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811aeccc",
   "metadata": {},
   "source": [
    "# Comparison to Other Distributions\n",
    "We can observe that histogram of duplicates from uniform multinomal is practically identical to fitted binomial and fitted Poisson distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ee0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_uniform_multinomial_duplicate_histogram(n: int, k: int, num_replicates: int = 10) -> dict:\n",
    "    multinomial_pmf = np.ones(int(k)) / k\n",
    "    estimated_duplicates = defaultdict(int)\n",
    "    for replicate in range(num_replicates):        \n",
    "        sampled_counter = Counter(np.random.multinomial(n=n, pvals=multinomial_pmf))\n",
    "        for key, value in sampled_counter.items():\n",
    "            estimated_duplicates[key] += value\n",
    "    return {key: value / num_replicates for key, value in estimated_duplicates.items() if value / num_replicates >= 1}\n",
    "\n",
    "multinomial_duplicate_counter = estimate_uniform_multinomial_duplicate_histogram(n=median_cell_num_reads,\n",
    "                                                                         k=estimated_support_size)\n",
    "\n",
    "max_x_value_to_display = 10\n",
    "x = np.asarray(sorted(list(set(median_cell_duplicate_counter) | set(multinomial_duplicate_counter))))\n",
    "x = x[:max_x_value_to_display]\n",
    "\n",
    "y_median_cell = [median_cell_duplicate_counter[x] if x in median_cell_duplicate_counter else 0 for x in x]\n",
    "y_multinomial = np.asarray([multinomial_duplicate_counter[x] if x in multinomial_duplicate_counter else 0 for x in x])\n",
    "\n",
    "poisson_rv = stats.poisson(median_cell_num_reads / estimated_support_size)\n",
    "y_poisson = np.asarray([estimated_support_size * poisson_rv.pmf(x) for x in x])\n",
    "\n",
    "binomial_rv = stats.binom(median_cell_num_reads, 1/ estimated_support_size)\n",
    "y_binomial = np.asarray([estimated_support_size * binomial_rv.pmf(x) for x in x])\n",
    "\n",
    "\n",
    "bar_width = 0.22\n",
    "plt.bar(x=x - 3/2*bar_width, height=y_median_cell, width=bar_width, label='Data', color='tab:blue')\n",
    "plt.bar(x=0 - 3/2*bar_width, height=y_median_cell[0], width=bar_width, label='Data (estimate of unobserved)',\n",
    "        color='lightblue')\n",
    "plt.bar(x=x - 1/2*bar_width, height=y_multinomial, width=bar_width, label='Uniform multinomial histogram', color='tab:orange')\n",
    "plt.bar(x=x + 1/2 * bar_width, height=y_poisson, width=bar_width, label='Poisson', color='tab:green')\n",
    "plt.bar(x=x + 3/2*bar_width, height=y_binomial, width=bar_width, label='Binomial', color='tab:red')\n",
    "plt.legend()\n",
    "plt.title('Data vs other distributions')\n",
    "plt.xlabel('Num. duplicates')\n",
    "plt.ylabel('UMIs')\n",
    "plt.savefig('data_vs_other_distributions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1a9f5",
   "metadata": {},
   "source": [
    "# Goodness of Fit\n",
    "Compare the theoretical distributions (histogram of uniform multinomial, Poisson / Binomial) to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize Poisson, since we arbitralily cutted it at max_x_value_to_display = 10\n",
    "# (Poisson's support is whole N so some cut is necessary anyway)\n",
    "\n",
    "y_poisson *= sum(y_median_cell) / sum(y_poisson)\n",
    "\n",
    "godness_of_fit_chi_square  = stats.chisquare(f_obs=y_median_cell, f_exp=y_poisson)\n",
    "print(f\"{godness_of_fit_chi_square.pvalue=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce17b7b",
   "metadata": {},
   "source": [
    "# Dirichlet-multinomial estimation\n",
    "We will estimate the parameter $\\alpha$ of Dirichlet-multinomial distribution (see Redmine for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gamma_stirling(x: float) -> float:\n",
    "    # Uses Stirling approximation for larger value of x\n",
    "    return np.log(scipy.special.gamma(x)) if x <= 15 else np.log(np.sqrt(2 * np.pi * (x - 1))) + (x - 1) * np.log(\n",
    "        (x - 1) / np.e)\n",
    "\n",
    "\n",
    "def ll_dirichlet_multinomial(duplicate_counter: dict, alpha: float) -> float:\n",
    "    k = sum(duplicate_counter.values())\n",
    "    n = sum([key * value for key, value in duplicate_counter.items()])\n",
    "    ll = log_gamma_stirling(alpha * k) - log_gamma_stirling(n + alpha * k) - k * log_gamma_stirling(alpha)\n",
    "    for key, value in duplicate_counter.items():\n",
    "        ll += value * log_gamma_stirling(key + alpha)\n",
    "    return ll\n",
    "\n",
    "\n",
    "alphas = np.arange(0.01, 20, 0.01)\n",
    "log_likelihoods = [ll_dirichlet_multinomial(median_cell_duplicate_counter, alpha) for alpha in alphas]\n",
    "\n",
    "best_alpha = alphas[np.argmax(log_likelihoods)]\n",
    "print(f\"{best_alpha=}\")\n",
    "\n",
    "plt.plot(alphas, log_likelihoods)\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel('Log-likelihood (without const. term)')\n",
    "plt.title(\"Log likelihood of Dirichlet-multinomial\")\n",
    "plt.savefig('log_likelihood_dirichlet_multinomial.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c925e0",
   "metadata": {},
   "source": [
    "Given the estimated values of $k$ and $\\alpha$, we may model sequencing of varying depth $n$ simply by sampling from corresponding Dirichlet-multinomial distribution and aggregating the results over several such samples. When estimating the histogram of duplicates for the $n=51873$ (corresponding to the training data), we obtain a reasonably looking fit of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7577bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_dirichlet_multinomial_duplicate_histogram(n: int, k: int = 33_559, alpha: float = 1.65, num_replicates: int = 10) -> dict:\n",
    "    estimated_duplicates = defaultdict(int)\n",
    "    for replicate in range(num_replicates):\n",
    "        p = np.random.dirichlet(alpha * np.ones(k))\n",
    "        sampled_counter = Counter(np.random.multinomial(n=n, pvals=p))\n",
    "        for key, value in sampled_counter.items():\n",
    "            estimated_duplicates[key] += value\n",
    "    return {key: value / num_replicates for key, value in estimated_duplicates.items() if value / num_replicates >= 1}\n",
    "\n",
    "\n",
    "estimated_histogram = estimate_dirichlet_multinomial_duplicate_histogram(median_cell_num_reads)\n",
    "\n",
    "x = np.asarray(sorted(list(set(median_cell_duplicate_counter) | set(estimated_histogram))))\n",
    "y_duplicate = [median_cell_duplicate_counter[x] if x in median_cell_duplicate_counter else 0 for x in x]\n",
    "y_sampled = [estimated_histogram[x] if x in estimated_histogram else 0 for x in x]\n",
    "\n",
    "bar_width = 0.4\n",
    "plt.bar(x=x - bar_width / 2, height=y_duplicate, width=bar_width, label='Data', color='tab:blue')\n",
    "plt.bar(x=0 - bar_width / 2, height=y_duplicate[0], width=bar_width, label='Data (estimate of unobserved)',\n",
    "        color='lightblue')\n",
    "plt.bar(x=x + bar_width / 2, height=y_sampled, width=bar_width, label='Model fit', color='tab:orange')\n",
    "plt.legend()\n",
    "plt.title('Data vs fitted model')\n",
    "plt.xlabel('Num. duplicates')\n",
    "plt.ylabel('UMIs')\n",
    "plt.savefig('data_vs_model_fit.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13801799",
   "metadata": {},
   "source": [
    "# Comparison of All Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f65e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(max_x_value_to_display)\n",
    "y_dirichlet_multinomial = [estimated_histogram[x] if x in estimated_histogram else 0 for x in x]\n",
    "\n",
    "bar_width = 0.22\n",
    "plt.bar(x=x - 3/2*bar_width, height=y_median_cell, width=bar_width, label='Data', color='tab:blue')\n",
    "plt.bar(x=0 - 3/2*bar_width, height=y_median_cell[0], width=bar_width, label='Data (estimate of unobserved)',\n",
    "        color='lightblue')\n",
    "plt.bar(x=x - 1/2 * bar_width, height=y_dirichlet_multinomial, width=bar_width, label='Dirichlet multinomial histogram', color='tab:orange')\n",
    "plt.bar(x=x + 1/2*bar_width, height=y_multinomial, width=bar_width, label='Uniform multinomial histogram', color='tab:green')\n",
    "plt.bar(x=x + 3/2*bar_width, height=y_binomial, width=bar_width, label='Binomial', color='tab:red')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Data vs other distributions')\n",
    "plt.xlabel('Num. duplicates')\n",
    "plt.ylabel('UMIs')\n",
    "plt.savefig('data_vs_all_other_distributions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1530c66",
   "metadata": {},
   "source": [
    "# Prediction for Different Sequencing Depth\n",
    "We may use the model to predict the histogram of duplicates for different sequencing depth, e.g, $n=150,000$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9565ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_histogram = estimate_dirichlet_multinomial_duplicate_histogram(n=150_000)\n",
    "\n",
    "plt.bar(estimated_histogram.keys(), height=estimated_histogram.values(), label='Predicted count')\n",
    "plt.title('Model prediction for n=150,000')\n",
    "plt.ylabel('UMIs')\n",
    "plt.xlabel('Num. duplicates')\n",
    "plt.legend()\n",
    "plt.savefig('histogram_prediction_150k.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3980009",
   "metadata": {},
   "source": [
    "Given the model, we may also predict any statistic derived from the histogram. For example, if we would only use UMIs with at least 5 duplicate reads, we may predict the amount of such UMIs for different sequencing depths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = np.arange(10_000, 500_000, 5000)\n",
    "umis_with_enough_duplicates = []\n",
    "duplicate_threshold = 5\n",
    "for n in tqdm(depths, desc='Estimating UMIs with enough duplicates'):\n",
    "    estimated_histogram = estimate_dirichlet_multinomial_duplicate_histogram(n)\n",
    "    umis_with_enough_duplicates.append(sum([value for key, value in estimated_histogram.items() if \n",
    "                                            key >= duplicate_threshold]))\n",
    "\n",
    "plt.plot(depths, umis_with_enough_duplicates)\n",
    "plt.xlabel('Sequencing depth')\n",
    "plt.ylabel(fr'UMIs with $\\geq {duplicate_threshold}$ duplicate reads')\n",
    "plt.title(fr'UMIs with $\\geq {duplicate_threshold}$ duplicates vs. sequencing depth')\n",
    "plt.savefig('umis_with_enough_duplicates_vs_depth.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a6abd",
   "metadata": {},
   "source": [
    "If we assume that the cost of sequencing is linear in the number of reads (and does not depend directly on the number of cells being sequenced), we may want to select a sequencing depth that maximizes ratio of UMIs with enough duplicates to reads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_sequencing_depth = depths[np.argmax(umis_with_enough_duplicates / depths)]\n",
    "\n",
    "plt.plot(depths, umis_with_enough_duplicates / depths)\n",
    "plt.xlabel('Sequencing depth')\n",
    "plt.ylabel(fr'UMIs with $\\geq {duplicate_threshold}$ duplicates per read')\n",
    "plt.title(fr'UMIs with $\\geq {duplicate_threshold}$ duplicates per read vs. sequencing depth')\n",
    "plt.vlines(x=optimal_sequencing_depth, ymin=0, ymax=np.max(umis_with_enough_duplicates / depths), \n",
    "           color = 'grey', ls= 'dashed')\n",
    "plt.savefig('umis_with_enough_duplicates_per_read_vs_depth.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"{optimal_sequencing_depth=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
